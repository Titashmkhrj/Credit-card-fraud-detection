{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Credit_card_fraud_HPO_MS_Testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1LNEYDIB5XThvXFYduhUdM-PwpmxMK38L",
      "authorship_tag": "ABX9TyNzWYkZbaivGW4atNgJh/Es",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Titashmkhrj/Credit-card-fraud-detection/blob/master/Credit_card_fraud_HPO_MS_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy3adweSMu8_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "outputId": "271efdaa-addf-4764-f31f-407eb9dfa397"
      },
      "source": [
        "# importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "import imblearn \n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.linear_model import (LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import (RandomizedSearchCV, train_test_split, cross_val_score)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# models as per the sequence in the parameter grid \n",
        "model_objects = [LogisticRegression(),\n",
        "                 LogisticRegression(),\n",
        "\t\t\t\t\t\t\t\t LogisticRegression(),\n",
        "\t\t\t\t\t\t\t\t PassiveAggressiveClassifier(),\n",
        "\t\t\t\t\t\t\t\t RidgeClassifier(),\n",
        "\t\t\t\t\t\t\t\t KNeighborsClassifier(),\n",
        "\t\t\t\t\t\t\t\t SVC(),\n",
        "\t\t\t\t\t\t\t\t DecisionTreeClassifier(),\n",
        "\t\t\t\t\t\t\t\t RandomForestClassifier()]\n",
        "\n",
        "\n",
        "\n",
        "# hyoer-parameter dictionary for the tunningof the models\n",
        "parameter_grid = {'LR_l1' : {'model__penalty' : ['l1'],\n",
        "                              'model__C' : [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "                              'model__random_state' : [42],\n",
        "                              'model__solver' : ['liblinear', 'saga'],\n",
        "                              'model__max_iter' : [100000]\n",
        "                          },\n",
        "\t\t\t\t\n",
        "                  'LR_l2' : {'model__penalty' : ['l2'],\n",
        "                              'model__C' : [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "                              'model__random_state' : [42],\n",
        "                              'model__solver' : ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
        "                              'model__max_iter' : [100000]\n",
        "                          },\n",
        "\n",
        "                  'LR_ElNet' : {'model__penalty' : ['elasticnet'],\n",
        "                                'model__l1_ratio' : [0.3, 0.5, 0.7],\n",
        "                                'model__C' : [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "                                'model__random_state' : [42],\n",
        "                                'model__solver' : ['saga'],\n",
        "                                'model__max_iter' : [100000]\n",
        "                              },\n",
        "\n",
        "                  'Pass_Agg_clif' : {'model__c' : [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "                                      'model__fit_intercept' : ['True', 'False'],\n",
        "                                      'model__random_state' : [42],\n",
        "                                      'model__loss' : ['hinge', 'squared_hinge'],\n",
        "                                      'model__class_weight' : ['balanced', None]\n",
        "                                  },\n",
        "                  \n",
        "                  'Ridge_clif' : {'model__alpha' : [500.0, 50.0, 5.0, 0.5, 0.05, 0.005],\n",
        "                                  'model__fit_intercept' : ['True', 'False'],\n",
        "                                  'model__normalize' : ['True', 'False'],\n",
        "                                  'model__class_weight' : ['balanced', None],\n",
        "                                  'model__solver' : ['svd', 'cholesky', 'lsqr', 'sparse_cg']\n",
        "                              },\n",
        "                  \n",
        "                  'KN_classif' : {'model__n_neighbor' : [2,4,6,8,10,],\n",
        "                                  'model__p' : [2,3,5]                     \n",
        "                              },\n",
        "                  \n",
        "                  'SVC' : {'model__c' : [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "                           'model__gamma' : ['scale', 'auto'],\n",
        "                           'model__class_weight' : ['balanced', None]                      \n",
        "                      },\n",
        "                  \n",
        "                  'DT_clif' : {'model__criterion': ['gini','entropy'],\n",
        "                                'model__max_features': ['sqrt','log2',None],\n",
        "                                'model__min_samples_leaf': [1,2,5,10],\n",
        "                                'model__min_samples_split' : [1,2,5,10,15,100],\n",
        "                                'model__max_depth': [5,8,15,25,30,None]\n",
        "                          },\n",
        "                  \n",
        "                  'RF_clif' : {'model__n_estimators' : [120,300,500,800,1200],\n",
        "                               'model__max_features': ['sqrt','log2',None],\n",
        "                                'model__min_samples_leaf': [1,2,5,10],\n",
        "                                'model__min_samples_split' : [1,2,5,10,15,100],\n",
        "                                'model__max_depth': [5,8,15,25,30,None]                      \n",
        "                          }\n",
        "              }\n",
        "\n",
        "\n",
        "\n",
        "# reading the feature and target spaces for our project\n",
        "x_data = pd.read_csv('/content/drive/My Drive/data_for_HPO&MS/credit_card_fraud/feature_space.csv')\n",
        "y_data = pd.read_csv('/content/drive/My Drive/data_for_HPO&MS/credit_card_fraud/target_space.csv')\n",
        "# dropping an unnecessary column from our target space\n",
        "y_data.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# splitting our dataset into train, validation and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.3, random_state = 42)\n",
        "x_optimization, x_validation, y_optimization, y_vaildation = train_test_split(x_train, y_train, test_size = 0.3, random_state = 42)\n",
        "\n",
        "\n",
        "print('Tunning the hyper-parameter...............')\n",
        "\n",
        "# initiating an empty list for storing the optimized models\n",
        "hyper_parameter_optimized_models = []\n",
        "\n",
        "# maing the objects for our hyper parameter search  and model selection pipelines\n",
        "over_sampler = SMOTE(random_state=42)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# initiating the random search\n",
        "for grid, model in zip(parameter_grid.values(), model_objects) :\n",
        "  classif_model = Pipeline([('resampler', over_sampler), ('scaler', scaler), ('model', model)]),\n",
        "  optimizer = RandomizedSearchCV(estimator = classif_model,\n",
        "\t\t\t\t\t\t\t\tparam_distributions = grid,\n",
        "\t\t\t\t\t\t\t\trandom_state = 42,\n",
        "\t\t\t\t\t\t\t\tcv = 3,\n",
        "\t\t\t\t\t\t\t\terror_score = -1,\n",
        "\t\t\t\t\t\t\t\tverbose = 10\n",
        "\t\t\t\t\t\t\t\t)\n",
        "  optimizer.fit(x_optimization, y_optimization)\n",
        "\t# appending the best estimator to a list\n",
        "  hyper_parameter_optimized_models.append(optimizer.best_estimator_)\n",
        "\n",
        "print('Hyper parameter tunning is finished.')\n",
        "\n",
        "# initiating the model selection proess\n",
        "print('Model selection .........')\n",
        "\n",
        "# initiating an empty list to stre the validation scores of the optimized models\n",
        "optimized_model_validation_scores = []\n",
        "\n",
        "for optimized_model in hyper_parameter_optimized_models :\n",
        "  optimized_model_pipeline = Pipeline([('resampler', over_sampler), ('scaler', scaler), ('optimized_model', optimized_model)])\n",
        "  model_validation_scores = cross_val_score(optimized_model_pipeline, x_validation, y_validation.ravel(), cv=3)\n",
        "  optimized_model_validation_scores.append(np.mean(model_validation_scores))\n",
        "\n",
        "# making a dictionary to store the results of the hyper-parameter optimization and the model selection process.\n",
        "results_dict = {'optimized_model':hyper_parameter_optimized_models,\n",
        "\t\t\t\t\t\t\t\t'validation_score':optimized_model_validation_scores\n",
        "\t\t\t\t\t\t}\n",
        "\n",
        "optimized_model_results = pd.DataFrame(results_dict)\n",
        "# saving the results of the hyper-parameter optimization and model_selection in a csv file\n",
        "optimized_model_results.to_csv('/content/drive/My Drive/data_for_HPO&MS/credit_card_fraud/model_optimizaion_report.csv')\n",
        "print('Model selection is finished')\n",
        "\n",
        "\n",
        "print('Initiating the process of our final phase to judge the average out-of-sample performance of our best found optimized model.')\n",
        "# selectin gthe best model by its index for the final predictions\n",
        "best_model_idx = optimized_model_results['validation_score'].idxmax(axis=0)\n",
        "best_model = optimized_model_results.iloc[best_model_idx,0]\n",
        "\n",
        "print('The best model to our finding is ', best_model)\n",
        "\n",
        "'''\n",
        "                                                      -------------------------------\n",
        "                                                              FINAL PREDICTION\n",
        "                                                      -------------------------------\n",
        "we are utilizing the whole training dataset for this purpose.\n",
        "resampling our training datasets, in order to prevent overfitting of our models on the majority class of the target feature in our training set\n",
        "'''\n",
        "\n",
        "x_train_resampled, y_train_resampled = SMOTE(random_state=42).fit_resample(x_train, y_train)\n",
        "# scaling our features in the training dataset\n",
        "scaler = StandardScaler().fit(x_train_resampled)\n",
        "x_train_scaled = scaler.transform(x_train_resampled)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "# re-fitting out best found optimized model to the whole training set\n",
        "best_model.fit(x_train_scaled, y_train_resampled)\n",
        "out_of_sample_predictions = best_model.predict(x_test_scaled)\n",
        "\n",
        "final_score = accuracy_score(y_test, out_of_sample_predictions)\n",
        "\n",
        "print('The final average out-of-sample performance score of our best optimized model is', final_score)\n",
        "\n",
        "# saving our best found optimized model for this data, as a pickle file\n",
        "joblib.dump(best_model, '/content/drive/My Drive/data_for_HPO&MS/credit_card_fraud/best_model.pkl') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tunning the hyper-parameter...............\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ead418d13127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m                                                                 \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \t\t\t\t\t\t\t\t)\n\u001b[0;32m--> 129\u001b[0;31m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_optimization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_optimization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;31m# appending the best estimator to a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0mhyper_parameter_optimized_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         scorers, self.multimetric_ = _check_multimetric_scoring(\n\u001b[0;32m--> 629\u001b[0;31m             self.estimator, scoring=self.scoring)\n\u001b[0m\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultimetric_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_check_multimetric_scoring\u001b[0;34m(estimator, scoring)\u001b[0m\n\u001b[1;32m    471\u001b[0m     if callable(scoring) or scoring is None or isinstance(scoring,\n\u001b[1;32m    472\u001b[0m                                                           str):\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscorers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         raise TypeError(\"estimator should be an estimator implementing \"\n\u001b[0;32m--> 401\u001b[0;31m                         \"'fit' method, %r was passed\" % estimator)\n\u001b[0m\u001b[1;32m    402\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: estimator should be an estimator implementing 'fit' method, Pipeline(memory=None,\n         steps=[('resampler',\n                 SMOTE(k_neighbors=5, kind='deprecated',\n                       m_neighbors='deprecated', n_jobs=1,\n                       out_step='deprecated', random_state=42, ratio=None,\n                       sampling_strategy='auto', svm_estimator='deprecated')),\n                ('scaler',\n                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n                ('model',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='auto', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='lbfgs', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False) was passed"
          ]
        }
      ]
    }
  ]
}